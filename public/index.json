[
{
	"uri": "http://localhost:1313/internworkshop/",
	"title": "API-First Development",
	"tags": [],
	"description": "",
	"content": "API-First Development with OpenAPI and Code Generation Overall In this lab, you will learn the basic and practices of API Gateway, Lambda functions, CodeBuild, and CodePipeline. Practice deploying api with OpenAPI as well as automating the deployment process and testing code after pushing it to github.\nContent Introduction Preparation Connect to EC2 instance API-First Development logs Port Forwarding Clean up resources "
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/3.1-public-instance/",
	"title": "Connect to Public Instance",
	"tags": [],
	"description": "",
	"content": "\nGo to EC2 service management console. Click on Public Linux Instance. Click Actions. Click Security. Click Modify IAM role. At the Modify IAM role page. Click to select SSM-Role. Click Save. You will need to wait about 10 minutes before performing the next step. This time our EC2 instance will automatically register with the Session Manager.\nGo to the AWS Systems Manager service management console Drag the left menu slider down. Click Session Manager. Click Start Session. Then select Public Linux Instance and click Start session to access the instance. Terminal will appear on the browser. Testing with the command sudo tcpdump -nn port 22 and sudo tcpdump we will see no SSH traffic but only HTTPS traffic. Above, we have created a connection to the public instance without opening SSH port 22, for better security, avoiding any attack to the SSH port.\nOne disadvantage of the above method is that we have to open the Security Group outbound at port 443 to the internet. Since it\u0026rsquo;s a public instance, it probably won\u0026rsquo;t be a problem, but if you want extra security, you can block port 443 to the internet and still use the Session Manager. We will go through this in the private instance section below.\nYou can click terminate to end the currently connected session before proceeding to the next step.\n"
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/3.2-private-instance/3.2.2-createvpcendpoint/3.2.2.1-endpointssm/",
	"title": "Create Endpoint ssm",
	"tags": [],
	"description": "",
	"content": "Create VPC Endpoint SSM Go to VPC service management console Click Endpoints. Click Create endpoint. At the Create endpoint page. In the Name tag field, enter SSM. In the Service Category section, select AWS Services. In the Service Name section, In the Service category section, select: AWS services In the Service Name section enter: SSM then select Service Name: com.amazonaws.ap-southeast-1.ssm. In the Service Name column, click com.amazonaws.ap-southeast-1.ssm. In the VPC section, select Lab VPC. Select the first AZ, then select the Lab Private Subnet subnet. Scroll down. In the Security Group section, select the Security group SG VPC Endpoint that we created earlier. In the Policy section, select Full access. Scroll down. Click Create endpoint. We have created the VPC Interface Endpoint for SSM. "
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/3.2-private-instance/3.2.1-enablevpcdns/",
	"title": "Enable DNS hostnames",
	"tags": [],
	"description": "",
	"content": "Enable DNS hostnames on VPC. To create VPC Endpoint we will need to enable DNS hostnames feature on VPC. Go to VPC service management console\nClick Your VPCs.\nSelect Lab VPC.\nClick Actions.\nClick Edit DNS hostnames.\nClick Endpoint, then click Create Endpoint.\nAt the Edit DNS hostnames page. Click to select Enable. Click Save changes. "
},
{
	"uri": "http://localhost:1313/internworkshop/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "In today’s modern software development landscape, designing and developing APIs is no longer a secondary task — it has become the core foundation of application architecture. With the API-First Development approach, entire systems are built around well-defined API specifications, ensuring consistency, reusability, and accelerated development speed.\nThis project implements a complete API-First development platform on AWS, integrating modern tools and services such as:\nOpenAPI Specification: Standardizes API design from the very beginning. Code Generation: Automatically generates server and client code from OpenAPI files. AWS Lambda \u0026amp; API Gateway: Deploys APIs using a serverless architecture. DynamoDB: Stores data using a scalable and efficient NoSQL model. CodePipeline \u0026amp; CodeBuild: Fully automates the CI/CD workflow. Jest \u0026amp; Coverage: Ensures quality through automated testing and coverage analysis. Redoc: Automatically generates clean, interactive API documentation from OpenAPI specs. Objectives Establish a modern API development workflow with:\nAPI-first design methodology Automatic code generation Integrated testing and documentation Deployment automation on AWS Why This Project? Reduces API development and maintenance time by up to 70% Resolves inconsistencies between code and documentation Builds a scalable, reusable platform for future projects "
},
{
	"uri": "http://localhost:1313/internworkshop/2-prerequiste/2.1-createdynamodb/",
	"title": "Prepare DynamoDB Table",
	"tags": [],
	"description": "",
	"content": "Create DynamoDB Table First, we need to create a database to store user information. DynamoDB is the optimal choice for serverless architecture because it can scale automatically and integrates well with Lambda.\nIn this step, we will create a DynamoDB table named Users with Partition key as userID (String)\nThe steps you need to complete for this step are as follows:\nGo to DynamoDB Console Click Create table Table name: Users Partition key: userID (String) Click Create table Go to DynamoDB Console: Access the AWS Management Console and search for DynamoDB. Click Create table: On the DynamoDB page, you will see the \u0026ldquo;Create table\u0026rdquo; button in the upper right corner. Fill in table information: Enter the table name as Users and Partition key as userID with String data type. Other parameters can be left as default. Finally, click Create table to complete. After creating the table, you will see the Users table in your DynamoDB tables list. You can click on the table name to view details and manage data in this table. "
},
{
	"uri": "http://localhost:1313/internworkshop/4-s3log/4.1-updateiamrole/",
	"title": "Update IAM Role",
	"tags": [],
	"description": "",
	"content": "For our EC2 instances to be able to send session logs to the S3 bucket, we will need to update the IAM Role assigned to the EC2 instance by adding a policy that allows access to S3.\nUpdate IAM Role Go to IAM service management console Click Roles. In the search box, enter SSM. Click on the SSM-Role role. Click Attach policies. In the Search box enter S3. Click the policy AmazonS3FullAccess. Click Attach policy. In the production environment, we will grant stricter permissions to the specified S3 bucket. In the framework of this lab, we use the policy AmazonS3FullAccess for convenience.\nNext, we will proceed to create an S3 bucket to store session logs.\n"
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/3.2-private-instance/",
	"title": "Connect to Private instance",
	"tags": [],
	"description": "",
	"content": "For Windows instance located in private subnet, there is no public IP, no internet gateway so it cannot go out internet.\nWith this type of instance, the traditional way is to use Bastion host technique which is expensive and laborious, but here we will use Session Manager with this type.\nBasically, the private instance still has to open the TCP 443 port to System Manager, but we don\u0026rsquo;t want to allow connection go out to the internet, but only in its VPC, to enhance our security posture.\nTo do that, we have to include the System Manager endpoint in the VPC, that is, using the VPC interface endpoint:\nVPC interface endpoint is attached to the subnet, so this method can be done not only with private subnet but also with public subnet, meaning that with public subnet, you can completely prohibit TCP 443 go out to the internet.\nContent: Enable DNS hostnames Create VPC Endpoint Connect Private Instance "
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/3.2-private-instance/3.2.2-createvpcendpoint/3.2.2.2-endpointssmmessages/",
	"title": "Create Endpoint ssmmessages",
	"tags": [],
	"description": "",
	"content": "Create VPC Endpoint SSMMESSAGES Go to VPC service management console Click Endpoints. Click Create endpoint. At the Create endpoint page. In the Name tag field, enter SSMMESSAGES. In the Service Category section, select AWS Services. In the Service Name section, In the Service category select: AWS services In the Service Name field enter: ssmmessages then select Service Name: com.amazonaws.ap-southeast-1.ssmmessages. In the Service Name column, click com.amazonaws.ap-southeast-1.ssmmessages. In the VPC section, select Lab VPC. Select the first AZ, then select the Lab Private Subnet subnet. Scroll down. In the Security Group section, select the Security group SG VPC Endpoint that we created earlier. In the Policy section, select Full access Scroll down. Click Create endpoint. We have created the VPC Interface Endpoint SSMMESSAGES. "
},
{
	"uri": "http://localhost:1313/internworkshop/2-prerequiste/2.2-createlambda/",
	"title": "Create Lambda Function",
	"tags": [],
	"description": "",
	"content": "1: Create Lambda Function In this step, we will create a Lambda function to handle requests from API Gateway. This Lambda function will perform CRUD (Create, Read, Update, Delete) operations on the DynamoDB table we created in the previous step. For simplicity, we use a single Lambda function to handle all CRUD requests. You can create different Lambda functions for each CRUD operation if you want.\nThe steps you need to complete for this step are as follows:\nGo to Lambda Console Click Create function Function name: UserService Runtime: Node.js 18.x Click Create function Go to Lambda Console: Access the AWS Management Console and search for Lambda. Click Create function: On the Lambda page, you will see the \u0026ldquo;Create function\u0026rdquo; button in the upper right corner. Fill in function information: Enter the function name as UserService, select Runtime as Node.js 18.x. Other parameters can be left as default. Finally, click Create function to complete.\nHere we use Node.js as the programming language for Lambda function. You can choose another language if you want, but make sure your source code is compatible with that runtime.\nSince I previously created a lambda function named UserService, I will name it UserService1 to avoid name conflicts. If you haven\u0026rsquo;t created one yet, you can name it UserService.\nBesides the name and runtime information, you can leave other parameters as default. After clicking Create function, Lambda will create the function for you. An IAM role will be automatically created to allow Lambda to access other AWS services like DynamoDB.\nClick Create function: After filling in all information, click the \u0026ldquo;Create function\u0026rdquo; button to complete creating the Lambda function. After creating the function, you will see the management page of the Lambda function UserService. Here, you can configure other parameters such as access permissions, environment variables, and function source code. 2: Upload Source Code for Lambda Function Now we will upload source code for the Lambda function UserService. This source code will include functions to handle CRUD requests from API Gateway and interact with DynamoDB.\nYou can upload source code directly to Lambda or use AWS CLI or AWS SDK to upload source code from your computer to Lambda.\nFor simplicity, we will add source code directly to Lambda.\nIn the Lambda function UserService management page, in the \u0026ldquo;Code\u0026rdquo; tab, you will see a source code editor. Copy and paste the following source code into the editor: const { DynamoDBClient } = require(\u0026#39;@aws-sdk/client-dynamodb\u0026#39;); const { DynamoDBDocumentClient, ScanCommand, GetCommand, PutCommand, UpdateCommand, DeleteCommand } = require(\u0026#39;@aws-sdk/lib-dynamodb\u0026#39;); const { randomUUID } = require(\u0026#39;crypto\u0026#39;); const client = new DynamoDBClient({}); const dynamodb = DynamoDBDocumentClient.from(client); const TABLE_NAME = \u0026#39;Users\u0026#39;; exports.handler = async (event) =\u0026gt; { console.log(\u0026#34;Full Event:\u0026#34;, JSON.stringify(event, null, 2)); console.log(\u0026#34;Event Received:\u0026#34;, { resource: event.resource, path: event.path, pathParameters: event.pathParameters, httpMethod: event.httpMethod }); const { httpMethod, pathParameters, body, resource } = event; try { switch (httpMethod) { case \u0026#39;GET\u0026#39;: if (resource === \u0026#39;/v1/users\u0026#39;) { return await getAllUsers(); } else if (resource === \u0026#39;/v1/users/{userID}\u0026#39; \u0026amp;\u0026amp; pathParameters?.userID) { return await getUserById(pathParameters.userID); } break; case \u0026#39;POST\u0026#39;: if (resource === \u0026#39;/v1/users\u0026#39;) { return await createUser(JSON.parse(body)); } break; case \u0026#39;PUT\u0026#39;: if (resource === \u0026#39;/v1/users/{userID}\u0026#39; \u0026amp;\u0026amp; pathParameters?.userID) { return await updateUser(pathParameters.userID, JSON.parse(body)); } break; case \u0026#39;DELETE\u0026#39;: if (resource === \u0026#39;/v1/users/{userID}\u0026#39; \u0026amp;\u0026amp; pathParameters?.userID) { return await deleteUser(pathParameters.userID); } break; } console.log(\u0026#34;No matching route found for:\u0026#34;, { httpMethod, resource, pathParameters }); return { statusCode: 404, body: JSON.stringify({ message: \u0026#39;Not Found\u0026#39; }) }; } catch (error) { console.error(\u0026#34;Lambda Error:\u0026#34;, { message: error.message, stack: error.stack }); return { statusCode: 500, body: JSON.stringify({ message: error.message || \u0026#39;Internal server error\u0026#39; }) }; } }; // ========== DynamoDB Functions ========== async function getAllUsers() { const result = await dynamodb.send(new ScanCommand({ TableName: TABLE_NAME })); return { statusCode: 200, body: JSON.stringify(result.Items) }; } async function getUserById(userID) { const result = await dynamodb.send(new GetCommand({ TableName: TABLE_NAME, Key: { userID } })); if (!result.Item) { return { statusCode: 404, body: JSON.stringify({ message: \u0026#39;User not found\u0026#39; }) }; } return { statusCode: 200, body: JSON.stringify(result.Item) }; } async function createUser(userData) { const userID = randomUUID(); const user = { userID, ...userData }; await dynamodb.send(new PutCommand({ TableName: TABLE_NAME, Item: user })); return { statusCode: 201, body: JSON.stringify(user) }; } async function updateUser(userID, userData) { const updateExpression = []; const expressionAttributeValues = {}; const expressionAttributeNames = {}; Object.keys(userData).forEach(key =\u0026gt; { updateExpression.push(`#${key} = :${key}`); expressionAttributeValues[`:${key}`] = userData[key]; expressionAttributeNames[`#${key}`] = key; }); try { const result = await dynamodb.send(new UpdateCommand({ TableName: TABLE_NAME, Key: { userID }, UpdateExpression: `SET ${updateExpression.join(\u0026#39;, \u0026#39;)}`, ExpressionAttributeValues: expressionAttributeValues, ExpressionAttributeNames: expressionAttributeNames, ConditionExpression: \u0026#39;attribute_exists(userID)\u0026#39;, ReturnValues: \u0026#39;ALL_NEW\u0026#39; })); return { statusCode: 200, body: JSON.stringify(result.Attributes) }; } catch (error) { if (error.name === \u0026#39;ConditionalCheckFailedException\u0026#39;) { return { statusCode: 404, body: JSON.stringify({ message: \u0026#39;User not found\u0026#39; }) }; } throw error; } } async function deleteUser(userID) { await dynamodb.send(new DeleteCommand({ TableName: TABLE_NAME, Key: { userID } })); return { statusCode: 204, body: \u0026#39;\u0026#39; }; } When you just added new source code, Lambda will notify that the source code has been changed and ask you to save it. Make sure this source code file is saved with the name index.js in the root directory of the Lambda function. Lambda will automatically use this file as the entry point for the function.\n3. Click Deploy to save changes. In this step, I added /v1/users to the beginning of the paths for easy management and API expansion in the future. You can change this path if you want, but remember to update it in the source code and API Gateway configuration later.\n3: Grant DynamoDB Access Permission to Lambda Function For the Lambda function to access the DynamoDB table, we need to grant access permission to this Lambda function. This is done through the IAM Role that the Lambda function uses.\nIn the Lambda function UserService management page, go to the \u0026ldquo;Configuration\u0026rdquo; tab, go to the \u0026ldquo;Permissions\u0026rdquo; tab, scroll down to the \u0026ldquo;Execution role\u0026rdquo; section.\nClick on the IAM role link in the \u0026ldquo;role name\u0026rdquo; section to open the IAM role management page. In the IAM role page, click Add permissions and select Attach policies. Search for and select the AmazonDynamoDBFullAccess policy to grant full access to DynamoDB. You can also create a custom policy if you want to limit access permissions. Click Attach policy to apply this policy to the IAM role of the Lambda function. Return to the Lambda function management page, you will see the IAM role has been updated with the new policy. "
},
{
	"uri": "http://localhost:1313/internworkshop/4-s3log/4.2-creates3bucket/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "In this step, we will create an S3 bucket to store session logs sent from EC2 instances.\nCreate S3 Bucket Access S3 service management console Click Create bucket. At the Create bucket page. In the Bucket name field, enter the bucket name lab-yourname-bucket-0001 In the Region section, select Region you are doing the current lab. The name of the S3 bucket must not be the same as all other S3 buckets in the system. You can substitute your name and enter a random number when generating the S3 bucket name.\nScroll down and click Create bucket. When we created the S3 bucket we did Block all public access so our EC2 instances won\u0026rsquo;t be able to connect to S3 via the internet. In the next step, we will configure the S3 Gateway Endpoint feature to allow EC2 instances to connect to the S3 bucket via the VPC\u0026rsquo;s internal network.\n"
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/3.2-private-instance/3.2.2-createvpcendpoint/",
	"title": "Create VPC Endpoint",
	"tags": [],
	"description": "",
	"content": "Create VPC Endpoint SSM We will create 3 interface endpoints required by the Session Manager:\nInterface endpoints: com.amazonaws.region.ssm com.amazonaws.region.ec2messages com.amazonaws.region.ssmmessages You can refer to more here\nContent: Create Endpoint ssm Create Endpoint ssmmessages Create Endpoint ec2messages "
},
{
	"uri": "http://localhost:1313/internworkshop/2-prerequiste/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "\rYou need to create a DynamoDB table, a Lambda function, and an API Gateway to complete this hands-on lab.\nSimple Architecture DynamoDB: Store user data Lambda Function: Handle CRUD operations API Gateway: REST API endpoints Below are the steps to create DynamoDB, Lambda, and API Gateway\nContent Create DynamoDB Table Create Lambda Function Create API Gateway "
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/",
	"title": "Connect to EC2 servers",
	"tags": [],
	"description": "",
	"content": "In this step, we will connect to our EC2 servers, located in both the public and private subnets.\nContent 3.1. Connect to EC2 Public Server 3.2. Cconnect to EC2 Private Server\n"
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/3.2-private-instance/3.2.3-connectec2/",
	"title": "Connect to instance",
	"tags": [],
	"description": "",
	"content": "Assign IAM role and restart EC2 instance. Go to EC2 service management console Click Private Windows Instance. Click Actions. Click Security. Click Modify IAM Role. At the Modify IAM Role page. In the IAM role section, select SSM-Role. Click Save. Click Private Windows Instance. Click Instance state. Click Reboot instance to restart, then click Reboot to confirm. Please wait 5 minutes before doing the next step.\nConnect to the private EC2 instance. Go to System Manager - Session Manager service management console Click Start session. Click Private Windows Instance. Click Start session. Type ipconfig command to check the IP address information of Private Windows Instance as shown below. "
},
{
	"uri": "http://localhost:1313/internworkshop/2-prerequiste/2.3-createapigatway/",
	"title": "Create API Gateway",
	"tags": [],
	"description": "",
	"content": "Create API Gateway In this step, we will create an API Gateway to connect with Lambda function and DynamoDB. API Gateway will provide endpoints to perform CRUD operations on the DynamoDB table through Lambda function.\nThe steps you need to complete for this step are as follows:\nGo to API Gateway Console Click Create API Select REST API and click Build Enter API information: API name: UserServiceAPI Description: API for User Service Endpoint Type: Regional Click Create API to complete creating the API Gateway. Go to API Gateway Console: Access the AWS Management Console and search for API Gateway. Click Create API: On the API Gateway page, you will see the \u0026ldquo;Create API\u0026rdquo; button in the upper right corner. Select REST API: Select the \u0026ldquo;REST API\u0026rdquo; option and click the \u0026ldquo;Build\u0026rdquo; button to start creating the API. Select New API: Select the \u0026ldquo;New API\u0026rdquo; option to create a new API. Fill in API information: Enter the API name as UserServiceAPI, description as API for User Service, and select endpoint type as Regional. Other parameters can be left as default. Finally, click Create API to complete. Create Resource and Method for API After creating the API, you will see the management page of API Gateway UserServiceAPI. Here, you can configure endpoints, HTTP methods, and integration with Lambda function. Select Create Resource: To create a resource for the API, you need to click the \u0026ldquo;Create Resource\u0026rdquo; button in the API management page. Since we will create a resource for users, you can name the resource users or v1/users to distinguish from other API versions in the future. Here, I will create a resource as v1/users to match the API version. First, you need to create a main resource for the API. This resource will be the root point for the API endpoints.\nFill in resource information: Enter the resource name as v1 and click Create Resource to create this resource. Similar to v1, we will create a child resource called users within the v1 resource. Select Create Method: After creating the v1/users resource, you need to create HTTP methods for this resource. Click the \u0026ldquo;Create Method\u0026rdquo; button to start. Select HTTP method: Choose the HTTP method you want to create for the v1/users resource. For example, you can choose GET, POST, PUT, and DELETE to perform CRUD operations. Here I will create GET, POST methods for the v1/users resource. Method type: Select the type of HTTP method you want to create. For example, if you want to create a GET method, select GET from the list. Proxy integration: Enable this option to use proxy integration. Integration type: Select the integration type for the method. In this case, you will select Lambda Function to integrate with Lambda function. Lambda Function: Select the name of the Lambda function you created in the previous step, for example UserService. This will link the HTTP method with the Lambda function to handle requests. You can select Lambda Function for all CRUD methods. You can leave the rest as default. Click Create Method to complete creating the method. We will create an additional resource v1/users/{userID} to get detailed information of a specific user. Similar to above, you can create the resource v1/users/{userID} by selecting Create Resource, entering the path as /v1/users/ and entering the resource name as {userID}.\nHere are the methods you will create for resources v1/users and v1/users/{userID}:\nGET /v1/users: Get a list of all users. POST /v1/users: Create a new user. GET /v1/users/{userID}: Get detailed information of a specific user. PUT /v1/users/{userID}: Update information of a specific user. DELETE /v1/users/{userID}: Delete a specific user. Deploy API After creating resources and methods for the API, you need to deploy the API to be able to use it. Deployment will create an endpoint that you can call from outside. Select Deploy API: Click the \u0026ldquo;Deploy API\u0026rdquo; button in the API management page. Select Deployment stage: Select or create a stage to deploy the API. You can create a new stage named dev or use the default stage. Fill in stage information: Enter the stage name as dev and click Deploy to complete deploying the API. After deployment, you will receive a URL endpoint for the API. This is the address you can use to call the API methods. Test API Before testing the API, you need to ensure that the Lambda function is configured correctly and has access to DynamoDB. You can check the IAM permissions of the Lambda function to ensure it has read and write permissions to the DynamoDB Users table.\nReturn to the Lambda function \u0026ldquo;Configuration\u0026rdquo; tab, select the \u0026ldquo;Permissions\u0026rdquo; tab and scroll down to the \u0026ldquo;resource-based policy\u0026rdquo; section. Here you will see the permissions that the Lambda function has been granted corresponding to the methods you created in API Gateway. You can test the API using Postman or curl to send requests to the endpoints you created. For example, add a new user by sending a POST request with user data in the body.\n{ \u0026#34;userID\u0026#34;: \u0026#34;12345\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;123 Main St\u0026#34; } Or to get a list of all users, you can send a GET request to the URL endpoint /v1/users. So we have completed creating API Gateway and configuring methods to interact with Lambda function and DynamoDB. Now you can use this API to perform CRUD operations on the Users table in DynamoDB through the Lambda function UserService.\n"
},
{
	"uri": "http://localhost:1313/internworkshop/3-accessibilitytoinstances/3.2-private-instance/3.2.2-createvpcendpoint/3.2.2.3-endpointec2messages/",
	"title": "Create Endpoint ec2messages",
	"tags": [],
	"description": "",
	"content": "Create VPC Endpoint EC2MESSAGES Go to VPC service management console Click Endpoints. Click Create endpoint. At the Create endpoint page. In the Name tag field, enter SSMMESSAGES. In the Service Category section, select AWS Services. In the Service Name section, In the Service category select: AWS services In the field Service Name enter: ec2 then select Service Name: com.amazonaws.ap-southeast-1.ec2messages. In the Service Name column, click com.amazonaws.ap-southeast-1.ec2messages. In the VPC section, select Lab VPC. Select the first AZ, then select the Lab Private Subnet subnet. Scroll down. In the Security Group section, select the Security group SG VPC Endpoint that we created earlier. In the Policy section, select Full access Scroll down. Click Create endpoint. We have created the VPC Interface Endpoint EC2MESSAGES.\nMake sure the 3 required endpoints have been created as shown below.\n"
},
{
	"uri": "http://localhost:1313/internworkshop/4-s3log/4.3-creategwes3/",
	"title": "Create S3 Gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Go to VPC service management console Click Endpoints. Click Create endpoint. At the Create endpoint page. In the Name tag field, enter S3GW. In the Service Category section, click AWS services. In the search box enter S3, then select com.amazonaws.[region].s3 In the Services section, select com.amazonaws.[region].s3 with the Type of Gateway. In the VPC section, select Lab VPC. In the Route tables section, select both route tables. Scroll down, click Create endpoint. The next step is to configure Session Manager to store session logs to the S3 bucket we created.\n"
},
{
	"uri": "http://localhost:1313/internworkshop/4-s3log/",
	"title": "API-First Development logs",
	"tags": [],
	"description": "",
	"content": "With API-First Development, we can view the history of API calls and deployments through API history. However, we have not seen the details of the commands used in a deployment.\nIn this section, we will proceed to create an S3 bucket and configure the API-First Development logs feature to see the details of the commands used in the deployment.\nContent: Update IAM Role Create S3 Bucket Create S3 Gateway endpoint Configure API-First Development logs "
},
{
	"uri": "http://localhost:1313/internworkshop/4-s3log/4.4-configsessionlogs/",
	"title": "Monitor session logs",
	"tags": [],
	"description": "",
	"content": "Monitor session logs Access System Manager - Session Manager service management console Click the Preferences tab. Click Edit. Scroll down, at S3 logging, click Enable. Uncheck Allow only encrypted S3 buckets. Click Choose a bucket name from the list. Select the S3 bucket you created. Scroll down, click Save to save the configuration.\nAccess System Manager - Session Manager service management console\nClick Start session. Click Private Windows Instance. Click Start session. Type the command ipconfig. Type the command hostname. Click Terminate to exit the session, click Terminate again to confirm. Check Session logs in S3 Go to S3 service management console Click on the name of the S3 bucket we created for the lab. Click on the object name sessions log On the objects detail page, click Open. Object logs will be opened in a new tab in the browser. You can view the stored commands in session logs. "
},
{
	"uri": "http://localhost:1313/internworkshop/5-portfwd/",
	"title": "Port Forwarding",
	"tags": [],
	"description": "",
	"content": "\rPort Forwarding is a useful way to redirect network traffic from one IP address - Port to another IP address - Port. With Port Forwarding we can access an EC2 instance located in the private subnet from our workstation.\nWe will configure Port Forwarding for the RDP connection between our machine and Private Windows Instance located in the private subnet we created for this exercise.\nCreate IAM user with permission to connect SSM Go to IAM service management console Click Users , then click Add users. At the Add user page. In the User name field, enter Portfwd. Click on Access key - Programmatic access. Click Next: Permissions. Click Attach existing policies directly.\nIn the search box, enter ssm. Click on AmazonSSMFullAccess. Click Next: Tags, click Next: Reviews. Click Create user. Save Access key ID and Secret access key information to perform AWS CLI configuration.\nInstall and Configure AWS CLI and Session Manager Plugin To perform this hands-on, make sure your workstation has AWS CLI and Session Manager Plugin installed -manager-working-with-install-plugin.html)\nMore hands-on tutorials on installing and configuring the AWS CLI can be found here.\nWith Windows, when extracting the Session Manager Plugin installation folder, run the install.bat file with Administrator permission to perform the installation.\nImplement Portforwarding Run the command below in Command Prompt on your machine to configure Port Forwarding. aws ssm start-session --target (your ID windows instance) --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region (your region) Windows Private Instance Instance ID information can be found when you view the EC2 Windows Private Instance server details.\nExample command: C:\\Windows\\system32\u0026gt;aws ssm start-session --target i-06343d7377486760c --document-name AWS-StartPortForwardingSession --parameters portNumber=\u0026#34;3389\u0026#34;,localPortNumber=\u0026#34;9999\u0026#34; --region ap-southeast-1 If your command gives an error like below: SessionManagerPlugin is not found. Please refer to SessionManager Documentation here: http://docs.aws.amazon.com/console/systems-manager/session-manager-plugin-not-found\nProve that you have not successfully installed the Session Manager Plugin. You may need to relaunch Command Prompt after installing Session Manager Plugin.\nConnect to the Private Windows Instance you created using the Remote Desktop tool on your workstation. In the Computer section: enter localhost:9999. Return to the administration interface of the System Manager - Session Manager service. Click tab Session history. We will see session logs with Document name AWS-StartPortForwardingSession. Congratulations on completing the lab on how to use Session Manager to connect and store session logs in S3 bucket. Remember to perform resource cleanup to avoid unintended costs.\n"
},
{
	"uri": "http://localhost:1313/internworkshop/6-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete EC2 instance Go to EC2 service management console\nClick Instances. Select both Public Linux Instance and Private Windows Instance instances. Click Instance state. Click Terminate instance, then click Terminate to confirm. Go to IAM service management console\nClick Roles. In the search box, enter SSM. Click to select SSM-Role. Click Delete, then enter the role name SSM-Role and click Delete to delete the role. Click Users. Click on user Portfwd. Click Delete, then enter the user name Portfwd and click Delete to delete the user. Delete S3 bucket Access System Manager - Session Manager service management console.\nClick the Preferences tab. Click Edit. Scroll down. In the section S3 logging. Uncheck Enable to disable logging. Scroll down. Click Save. Go to S3 service management console\nClick on the S3 bucket we created for this lab. (Example: lab-fcj-bucket-0001 ) Click Empty. Enter permanently delete, then click Empty to proceed to delete the object in the bucket. Click Exit. After deleting all objects in the bucket, click Delete\nEnter the name of the S3 bucket, then click Delete bucket to proceed with deleting the S3 bucket. Delete VPC Endpoints Go to VPC service management console Click Endpoints. Select the 4 endpoints we created for the lab including SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. In the confirm box, enter delete.\nClick Delete to proceed with deleting endpoints. Click the refresh icon, check that all endpoints have been deleted before proceeding to the next step.\nDelete VPC Go to VPC service management console\nClick Your VPCs. Click on Lab VPC. Click Actions. Click Delete VPC. In the confirm box, enter delete to confirm, click Delete to delete Lab VPC and related resources.\n"
},
{
	"uri": "http://localhost:1313/internworkshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/internworkshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]